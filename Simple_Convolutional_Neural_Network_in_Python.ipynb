{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network in Python - Step-by-Step\n",
    "\n",
    "This notebook contains the step-by-step building blocks of a simple convolutional neural network.\n",
    "\n",
    "**Functions**\n",
    "\n",
    "- Convolution functions, including:\n",
    "    - Zero Padding\n",
    "    - Convolve window \n",
    "    - Convolution forward\n",
    "    - Convolution backward\n",
    "    \n",
    "- Pooling functions, including:\n",
    "    - Pooling forward\n",
    "    - Create mask \n",
    "    - Distribute value\n",
    "    - Pooling backward\n",
    "    \n",
    "All functions are implemented from scratch in `numpy`. \n",
    "\n",
    "**Notation**:\n",
    "\n",
    "Superscript $[l]$ denotes an object of the $l^{th}$ layer e.g. $a^{[4]}$ is the $4^{th}$ layer activation; $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.\n",
    "\n",
    "\n",
    "Superscript $(i)$ denotes an object from the $i^{th}$ example e.g. $x^{(i)}$ is the $i^{th}$ training example input.\n",
    "    \n",
    "    \n",
    "Lowerscript $i$ denotes the $i^{th}$ entry of a vector e.g. $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$, assuming this is a fully connected (FC) layer.\n",
    "    \n",
    "    \n",
    "$n_H$, $n_W$ and $n_C$ denote respectively the height, width and number of channels of a given layer.\n",
    "\n",
    "\n",
    "$n_{H_{prev}}$, $n_{W_{prev}}$ and $n_{C_{prev}}$ denote respectively the height, width and number of channels of the previous layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Padding\n",
    "\n",
    "Zero-padding adds zeros around the border of an image; the main benefits of doing this are: \n",
    "- It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer. \n",
    "\n",
    "- It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \n",
    "    X_pad = np.pad(X, ((0,0), (pad, pad), (pad, pad), (0,0)), 'constant')\n",
    "    \n",
    "    return X_pad\n",
    "\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X\n",
    "    \n",
    "    Argument:\n",
    "    X = python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad = padding\n",
    "    \n",
    "    Returns:\n",
    "    X_pad = padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Step of Convolution \n",
    "\n",
    "Applies the filter to a single position of the input.\n",
    "\n",
    "- Takes an input volume \n",
    "- Applies a filter at every position of the input\n",
    "- Outputs another volume (usually of different size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "    # Element-wise product between a_slice and W\n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    \n",
    "    # Sum over all entries of the volume s\n",
    "    Z = np.sum(s)\n",
    "    \n",
    "    # Add bias b (cast to a float) to Z\n",
    "    Z = Z + float(b)\n",
    "\n",
    "    return Z\n",
    "\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev = slice of input data; shape = (f, f, n_C_prev)\n",
    "    W = weight parameters contained in a window; shape = (f, f, n_C_prev)\n",
    "    b = bias parameters contained in a window; shape = (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z = result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Forward Pass\n",
    "\n",
    "- Convolves many filters on the input\n",
    "- Each convolution results in a 2D matrix output\n",
    "- Outputs are stacked to get a 3D volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \n",
    "    # Retrieve dimensions \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume\n",
    "    n_H = int(((n_H_prev - f + (2 * pad)) / stride) + 1)\n",
    "    n_W = int(((n_W_prev - f + (2 * pad)) / stride) + 1)\n",
    "\n",
    "    \n",
    "    # Initialize the output volume Z with zeros\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(0, m):                             # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i, :, :, :]           # Select ith training example's padded activation\n",
    "        for h in range(0, n_H):                       # loop over vertical axis of the output volume\n",
    "            for w in range(0, n_W):                   # loop over horizontal axis of the output volume\n",
    "                for c in range(0, n_C):               # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = stride * h\n",
    "                    vert_end = (stride * h) + f\n",
    "                    horiz_start = stride * w\n",
    "                    horiz_end = (stride * w) + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
    "                                        \n",
    "    # Make sure output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev = output activations of the previous layer; shape = (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W = weights; sshape = (f, f, n_C_prev, n_C)\n",
    "    b = Biases; shape = (1, 1, 1, n_C)\n",
    "    hparameters = python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z = conv output; shape = (np.floor((n_H_prev - f - (2 * pad)) / stride) + 1)\n",
    "    cache = cache of values needed for the conv_backward() function\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pooling layer \n",
    "\n",
    "The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helping make feature detectors more invariant to its position in the input. The two types of pooling layer are: \n",
    "\n",
    "- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n",
    "\n",
    "- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n",
    "\n",
    "\n",
    "These pooling layers have no parameters for backpropagation to train; however, they have hyperparameters such as the window size $f$ - this specifies the height and width of the fxf window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "\n",
    "    # Retrieve dimensions\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(0, m):                           # loop over the training examples\n",
    "        for h in range(0, n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(0, n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (0, n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = stride*h\n",
    "                    vert_end = (stride*h) + f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = (stride*w) + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Make sure output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev = input data; shape = (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters = python dictionary containing \"f\" and \"stride\"\n",
    "    mode = pooling mode defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A = output of the pool layer; shape = (m, n_H, n_W, n_C)\n",
    "    cache = cache used in the backward pass of the pooling layer; contains the input and hparameters \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In modern deep learning frameworks you only have to implement the forward pass and the framework takes care of the pack pass, so most deep learning engineers don't need to bother with the details of backpropagation. It is, however, included below for completeness for those interested in understanding what is contained within those black boxes.\n",
    "\n",
    "## Convolutional Layer Backward Pass \n",
    "\n",
    "This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:\n",
    "\n",
    "\n",
    "$$dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw}$$\n",
    "\n",
    "\n",
    "Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the $h^{th}$ row and $w^{th}$ column (corresponding to the dot product taken at the $i^{th}$ stride left and $j^{th}$ stride down). Note that at each time, we multiply the the same filter $W_c$ by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices. \n",
    "\n",
    "This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:\n",
    "\n",
    "$$dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}$$\n",
    "\n",
    "Where $a_{slice}$ corresponds to the slice which was used to generate the acitivation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$. \n",
    "\n",
    "This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n",
    "\n",
    "$$db = \\sum_h \\sum_w dZ_{hw}$$\n",
    "\n",
    "db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "\n",
    "    # Retrieve information\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with correct shapes\n",
    "    dA_prev = np.zeros(A_prev.shape)                           \n",
    "    dW = np.zeros(W.shape)\n",
    "    db = np.zeros(b.shape)\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    # Update gradients for the window and filter's parameters\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] +=  W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    # Make sure output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ = gradient of the cost with respect to the output of the conv layer (Z); shape = (m, n_H, n_W, n_C)\n",
    "    cache = cache of values needed for the conv_backward(); output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev = gradient of the cost with respect to the input of the conv layer (A_prev); shape = (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW = gradient of the cost with respect to the weights of the conv layer (W); shape = (f, f, n_C_prev, n_C)\n",
    "    db = gradient of the cost with respect to the biases of the conv layer (b); shape = (1, 1, 1, n_C)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer Backward Pass\n",
    "\n",
    "Next is the implementation for the backward pass of the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagate the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. \n",
    "\n",
    "## Max Pooling - Backward Pass  \n",
    "\n",
    "The code in the cell below provides a helper function called `create_mask_from_window()` that creates a mask matrix which helps keep track of where the maximum of the matrix is.\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "Since this is the value that ultimately influenced the output and therefore cost, this is the value that backprop will propagate the gradient back to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \n",
    "    mask = x == np.max(x)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x to identify the max entry of x\n",
    "    \n",
    "    Arguments:\n",
    "    x = Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask = Array of the same shape as window; contains a True at the position corresponding to the max entry of x\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(2,3)\n",
    "\n",
    "mask = create_mask_from_window(x)\n",
    "\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling - Backward Pass \n",
    "\n",
    "The code in the cell below provides a helper function for average pooling.  Since every element of the input window in average pooling has an equal influence on the output, the mask needs to be different to that used for max pooling.\n",
    "\n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute value to distribute on the matrix\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a\n",
    "\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz = input scalar\n",
    "    shape = the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a = Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together: Pooling Backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \n",
    "    # Retrieve information\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        \n",
    "                        # Create the mask from a_prev_slice\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        \n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA \n",
    "                        da = dA[i, h, w, c]\n",
    "                        \n",
    "                        # Define the shape of the filter as fxf\n",
    "                        shape = (f, f)\n",
    "                        \n",
    "                        # Distribute it to get the correct slice of dA_prev\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    # Make sure output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    return dA_prev\n",
    "\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA = gradient of cost with respect to the output of the pooling layer; same shape as A\n",
    "    cache = cache output from the forward pass of the pooling layer; contains the layer's input and hparameters \n",
    "    mode = the pooling mode defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev = gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
