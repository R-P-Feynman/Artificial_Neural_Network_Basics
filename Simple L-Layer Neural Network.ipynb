{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Activation Helper Functions\n",
    "\n",
    "- Sigmoid\n",
    "- Relu\n",
    "- Relu_backward\n",
    "- Sigmoid_backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "    \"\"\"\n",
    "    Sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z = numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A = output of sigmoid(z)\n",
    "    cache = returns Z, too; used during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "    \"\"\"\n",
    "    RELU function\n",
    "    \n",
    "    Arguments:\n",
    "    Z = output of the linear layer of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A = post-activation parameter\n",
    "    cache = a python dictionary containing A; used during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # convert dz to correct object\n",
    "    \n",
    "    dZ[Z <= 0] = 0  # When z <= 0, set dz to 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "    \"\"\"\n",
    "    Backward propagation for a single RELU unit\n",
    "    \n",
    "    Arguments:\n",
    "    dA = post-activation gradient of any shape\n",
    "    cache = Z; used during backpropagation\n",
    "    \n",
    "    Returns:\n",
    "    dZ = Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "    \"\"\"\n",
    "    Backward propagation for a single SIGMOID unit\n",
    "    \n",
    "    Arguments:\n",
    "    dA = post-activation gradient of any shape\n",
    "    cache = Z; used during backpropagation\n",
    "    \n",
    "    Returns:\n",
    "    dZ = Gradient of the cost with respect to Z\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Initialise L-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "\n",
    "    parameters = {}\n",
    "    \n",
    "    L = len(layer_dims)            # number of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims = python array (list) containing the dimensions of each layer in\n",
    "    \n",
    "    Returns:\n",
    "    parameters = python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl = weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl = bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Forward Propagation Module\n",
    "\n",
    "### 4.1 - Linear Forward \n",
    "\n",
    "The linear forward module (vectorized over all the examples) computes the following equations:\n",
    "\n",
    "$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$\n",
    "\n",
    "where $A^{[0]} = X$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A = activations from previous layer (or input data); shape = (size of previous layer, number of examples)\n",
    "    W = weights matrix; numpy array shape = (size of current layer, size of previous layer)\n",
    "    b = bias vector; numpy array shape = (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z = the input of the activation function, also called pre-activation parameter \n",
    "    cache = a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Linear Activation Forward\n",
    "\n",
    "Forward propagation of the *LINEAR->ACTIVATION* layer.\n",
    "\n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs = A_prev, W, b; outputs = A, activation_cache\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs = A_prev, W, b; outputs = A, activation_cache\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A = activations from previous layer (or input data); shape = (size of previous layer, number of examples)\n",
    "    W = weights matrix; numpy array shape = (size of current layer, size of previous layer)\n",
    "    b = bias vector; numpy array shape = (size of the current layer, 1)\n",
    "    activation = the activation to be used in this layer - sigmoid or relu\n",
    "\n",
    "    Returns:\n",
    "    A = the output of the activation function \n",
    "    cache = a python dictionary containing 'linear_cache' and 'activation_cache'; used in backpropagation\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - L-Layer Model  Forward\n",
    "\n",
    "(L-1) layers of ReLU + one sigmoid.\n",
    "\n",
    "`AL` denotes $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers\n",
    "    \n",
    "    # LINEAR -> RELU *(L-1)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # LINEAR -> SIGMOID\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X = data; numpy array shape = (input size, number of examples)\n",
    "    parameters = output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL = last post-activation value\n",
    "    caches = list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function\n",
    "\n",
    "Cross-entropy cost $J$: \n",
    "\n",
    "$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from AL and y.\n",
    "    cost = -(1/m) * np.sum(np.dot(Y, np.log(AL).T) + np.dot((1 - Y), np.log(1 - AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # ensure shape of cost is correct\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL = probability vector corresponding to label predictions= shape = (1, number of examples)\n",
    "    Y = labels vector; shape = (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost = cross-entropy cost\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backpropagation\n",
    "\n",
    "6.1 = LINEAR backward\n",
    "\n",
    "6.2 = LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "\n",
    "6.3 = LINEAR -> RELU $\\times$ (L-1) -> LINEAR -> SIGMOID backward\n",
    "### 6.1 - Linear Backward\n",
    "\n",
    "$dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$\n",
    "\n",
    "$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$\n",
    "\n",
    "$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$\n",
    "\n",
    "$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ = gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache = tuple of values (A_prev, W, b) from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev = gradient of the cost with respect to the activation (of the previous layer l-1); same shape as A_prev\n",
    "    dW = gradient of the cost with respect to W (current layer l); same shape as W\n",
    "    db = gradient of the cost with respect to b (current layer l); same shape as b\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation Backward\n",
    "\n",
    "$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA = post-activation gradient for current layer l \n",
    "    cache = tuple of values (linear_cache, activation_cache); used for backpropagation\n",
    "    activation = the activation to be used in this layer - sigmoid or relu\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev = gradient of the cost with respect to the activation (of the previous layer l-1); same shape as A_prev\n",
    "    dW = gradient of the cost with respect to W (current layer l); same shape as W\n",
    "    db = gradient of the cost with respect to b (current layer l); same shape as b\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Layer Model Backward \n",
    "\n",
    "Backpropagation for the LINEAR->RELU $\\times$ (L-1) -> LINEAR -> SIGMOID model.\n",
    "\n",
    "`dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "\n",
    "\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # make Y the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer SIGMOID -> LINEAR gradients\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\") \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer RELU -> LINEAR gradients\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation=\"relu\") \n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL = probability vector; output of the forward propagation (L_model_forward())\n",
    "    Y = labels vector\n",
    "    caches = list of caches containing:\n",
    "                every cache of linear_activation_forward() with relu (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with sigmoid (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads = A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - Update Parameters\n",
    "\n",
    "\n",
    "Update the parameters using gradient descent: \n",
    "\n",
    "$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$\n",
    "\n",
    "$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters = python dictionary containing parameters \n",
    "    grads = python dictionary containing gradients; output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters = python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - L-Layer Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [13, 10, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU *(L-1) -> LINEAR -> SIGMOID\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backpropagation\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X = data; numpy array shape = (number of examples, num_px * num_px * 3)\n",
    "    Y = labels vector; shape = (1, number of examples)\n",
    "    layers_dims = list containing the input size and each layer size, of length (number of layers + 1)\n",
    "    learning_rate = learning rate of the gradient descent update rule\n",
    "    num_iterations = number of iterations of the optimization loop\n",
    "    print_cost = if True, print cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters = parameters learnt by the model - used for prediction\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = L_layer_model(X_train.T, y_train.T, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Predict Classifications of Test Data\n",
    "\n",
    "Prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "\n",
    "    # Computes probabilities using forward propagation and classifies to 0 or 1 using 0.5 as the threshold.\n",
    "    A2, cache = L_model_forward(X, parameters)\n",
    "    predictions = np.zeros((A2.shape))\n",
    "    for i in range(A2.shape[1]):\n",
    "        if A2[0, i] > 0.5:\n",
    "            predictions[0, i] = 1\n",
    "        else:\n",
    "            predictions[0, i] = 0\n",
    "\n",
    "    return predictions\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters = python dictionary containing your parameters \n",
    "    X = input data; size = (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions = vector of predictions\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign test_data to X test data\n",
    "\n",
    "test_data = X_test.T\n",
    "\n",
    "predictions = predict(parameters, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign Y to test data\n",
    "\n",
    "Y = y_test.T\n",
    "\n",
    "# calculate accuracy\n",
    "\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + \n",
    "                               np.dot(1- Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
